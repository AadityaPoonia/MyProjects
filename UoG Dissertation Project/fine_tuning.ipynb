{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca211e2b-ac46-4878-ab58-62afb9c4e2d1",
   "metadata": {},
   "source": [
    "# Section 4: Fine-Tuning using Saul LM 7B\n",
    "This section involves the fine-tuning of the Saul LM 7B model on your dataset.\n",
    "The model will be trained further on the preprocessed dataset to adapt to the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c30feb-f85d-420e-bd3a-2d92ab84c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Function to load JSON data line by line\n",
    "def load_json_lines(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load the JSON data containing case details\n",
    "f_cases_data = load_json_lines('./data/nia_cases_to_process.json')\n",
    "\n",
    "# Sort the cases by JudgmentDate in ascending order\n",
    "sorted_cases = sorted(f_cases_data, key=lambda x: datetime.strptime(x['JudgmentDate'], '%d/%m/%Y'))\n",
    "\n",
    "# Load prompt template from files\n",
    "with open(\"./prompts/prompt_with_nocontext.txt\", \"r\") as f:\n",
    "    prompt_with_nocontext = f.read().strip()\n",
    "\n",
    "# Function to format the prompt using the no-context template\n",
    "def format_prompt_nocontext(doc):\n",
    "    return prompt_with_nocontext.format(\n",
    "        Case_Inputs=json.dumps(doc.get('Case_Inputs', ''), indent=1)[2:-2]\n",
    "    )\n",
    "\n",
    "# Model and Tokenizer Setup\n",
    "access_token = \"Enter your hugging face token\" \n",
    "model_id = \"Equall/Saul-7B-Instruct-v1\" \n",
    "\n",
    "# Configure the model for 4-bit quantization for efficient memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization for better memory efficiency\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model with the specified configuration\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  \n",
    "    use_cache=False, \n",
    "    token=access_token \n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "hf_model.gradient_checkpointing_enable()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set the EOS token as the padding token\n",
    "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
    "hf_tokenizer.pad_token_id = hf_tokenizer.eos_token_id\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "train_data, temp_data = sorted_cases[:int(0.8*len(sorted_cases))], sorted_cases[int(0.8*len(sorted_cases)):]\n",
    "\n",
    "# Further split the remaining data into validation and test sets\n",
    "val_data, test_data = temp_data[:int(0.5*len(temp_data))], temp_data[int(0.5*len(temp_data)):]\n",
    "\n",
    "# Print the sizes of the training, validation, and test datasets\n",
    "print(f\"Training data: {len(train_data)} cases\")\n",
    "print(f\"Validation data: {len(val_data)} cases\")\n",
    "print(f\"Test data: {len(test_data)} cases\")\n",
    "\n",
    "# Convert the data into a DatasetDict format\n",
    "ft_dataset = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'validation': Dataset.from_list(val_data),\n",
    "    'test': Dataset.from_list(test_data),\n",
    "})\n",
    "\n",
    "# Extract judgment dates for visualization\n",
    "train_dates = [datetime.strptime(case['JudgmentDate'], '%d/%m/%Y') for case in train_data]\n",
    "val_dates = [datetime.strptime(case['JudgmentDate'], '%d/%m/%Y') for case in val_data]\n",
    "test_dates = [datetime.strptime(case['JudgmentDate'], '%d/%m/%Y') for case in test_data]\n",
    "\n",
    "# Plot the distribution of cases over time across training, validation, and test sets\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.hist(train_dates, bins=100, alpha=0.6, label=f'Train ({len(train_data)})', color='skyblue')\n",
    "plt.hist(val_dates, bins=10, alpha=0.7, label=f'Validation ({len(val_data)})', color='orange')\n",
    "plt.hist(test_dates, bins=10, alpha=0.7, label=f'Test ({len(test_data)})', color='orange')\n",
    "\n",
    "plt.xlabel('Judgment Date')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Cases on Time Axis')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a83c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for memory allocation configuration\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# Check and define the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the padding token in the model configuration\n",
    "hf_model.config.pad_token_id = hf_tokenizer.pad_token_id\n",
    "\n",
    "# Set up LoRA (Low-Rank Adaptation) configuration for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"] \n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model\n",
    "peft_model = get_peft_model(hf_model, peft_config)\n",
    "peft_model.train()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Function to tokenize the dataset for training\n",
    "def tokenize_function(case):\n",
    "    combined_input = format_prompt_nocontext(case)\n",
    "    result = hf_tokenizer(combined_input, truncation=True, max_length=4096, return_overflowing_tokens=True)\n",
    "    \n",
    "    # Handling labels similarly to inputs\n",
    "    labels = hf_tokenizer(case['Case_Result'], truncation=True, max_length=4096, return_overflowing_tokens=True)\n",
    "    \n",
    "    # Map the labels to the correct overflowed input chunks\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in labels.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    \n",
    "    result[\"labels\"] = result[\"input_ids\"]  \n",
    "    return result\n",
    "\n",
    "# Tokenize the entire dataset using the defined tokenize_function\n",
    "tokenized_datasets = ft_dataset.map(tokenize_function, batched=True, remove_columns=ft_dataset['train'].column_names)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data collator for dynamic padding during training\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=hf_tokenizer, model=hf_model, padding=True)\n",
    "\n",
    "# Load evaluation metrics\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    decoded_preds = hf_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = hf_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean the generated predictions\n",
    "    cleaned_preds = [pred.split(\"[/INST]\")[-1].strip() for pred in decoded_preds]   # Removing the prompt section from the generated result\n",
    "    \n",
    "    # Compute different evaluation metrics\n",
    "    bertscore_results = bertscore_metric.compute(predictions=cleaned_preds, references=decoded_labels, lang=\"en\")\n",
    "    rouge_results = rouge_metric.compute(predictions=cleaned_preds, references=decoded_labels)\n",
    "    meteor_results = meteor_metric.compute(predictions=cleaned_preds, references=decoded_labels)\n",
    "    \n",
    "    bertscore_f1 = np.mean(bertscore_results['f1'])\n",
    "    rouge_l_f1 = rouge_results['rougeL']\n",
    "    meteor_score = meteor_results['meteor']\n",
    "    \n",
    "    # Weighted score for overall evaluation\n",
    "    weights = {'bertscore': 0.1, 'rouge': 0.6, 'meteor': 0.3}\n",
    "    weighted_score = (weights['bertscore'] * bertscore_f1 +\n",
    "                      weights['rouge'] * rouge_l_f1 +\n",
    "                      weights['meteor'] * meteor_score)\n",
    "    \n",
    "    return {\n",
    "        'bertscore_f1': bertscore_f1,\n",
    "        'rouge_l_f1': rouge_l_f1,\n",
    "        'meteor': meteor_score,\n",
    "        'weighted_score': weighted_score\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Evaluate the model before training\n",
    "trainer.evaluate()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "peft_model.save_pretrained(\"./fine_tuned_model_v2\")\n",
    "hf_tokenizer.save_pretrained(\"./fine_tuned_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5148f-4df3-470a-94ad-c6bbe543cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Using the fine-tuned model to generate predictions\n",
    "\n",
    "# Ensuring torch device is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the tokenizer from the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model_v2\")\n",
    "\n",
    "# Load the fine-tuned model without moving it automatically to a specific device\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./fine_tuned_model_v2\",\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 to reduce memory footprint\n",
    "    device_map=\"auto\",  # Using Accelerate to manage device mapping\n",
    "    low_cpu_mem_usage=True  # Minimizes CPU memory usage during loading\n",
    ")\n",
    "\n",
    "# Function to generate analysis for a single case using the fine-tuned model\n",
    "def generate_single_analysis(case, model, tokenizer, device):\n",
    "    prompt = format_prompt_nocontext(case)\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096)\n",
    "    \n",
    "    # Ensure to send both input_ids and attention_mask to the correct device\n",
    "    tokens = {key: value.to(fine_tuned_model.device) for key, value in tokens.items()}\n",
    "\n",
    "    # Generate text while passing attention_mask and setting pad_token_id explicitly\n",
    "    generated_text = fine_tuned_model.generate(\n",
    "        input_ids=tokens['input_ids'],\n",
    "        attention_mask=tokens['attention_mask'],  \n",
    "        max_new_tokens=4096,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "\n",
    "results=[]\n",
    "references = []\n",
    "generated_texts = []\n",
    "\n",
    "# Perform inference on the test data\n",
    "for case in tqdm(test_data, desc='Generating Analysis'):\n",
    "    analysis = generate_single_analysis(case, fine_tuned_model, tokenizer, device).split(\"[/INST]\")[-1].strip()\n",
    "    case['Fine_Tuned_Generated_Analysis'] = analysis\n",
    "    results.append(case)\n",
    "    \n",
    "    # Storing reference and generated texts for evaluation\n",
    "    references.append(case['Case_Result'])\n",
    "    generated_texts.append(analysis)\n",
    "\n",
    "# Save the generated analyses to a JSON file\n",
    "with open('./results/fine_tuned_generated_analyses_v2.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Analysis Generated and Saved with Fine-Tuned Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
