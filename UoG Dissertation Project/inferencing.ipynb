{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6858e38f-753c-4840-a3dc-6fad8c6cb91d",
   "metadata": {},
   "source": [
    "# Section 3: Inference using Together AI\n",
    "This section focuses on loading a pre-trained model and using it for inference on the provided data.\n",
    "It includes loading the model and tokenizer, generating predictions, and processing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af53404-3d15-4f14-8cae-43c9fbf9c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install together\n",
    "\n",
    "import together as tg\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import evaluate\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Function to load data from a JSON file with each line containing a JSON object.\n",
    "def load_json_lines(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load the case data from a JSON file.\n",
    "f_cases_data = load_json_lines('./data/nia_cases_to_process.json')\n",
    "\n",
    "# Sort the cases by their judgment date in ascending order.\n",
    "sorted_cases = sorted(f_cases_data, key=lambda x: datetime.strptime(x['JudgmentDate'], '%d/%m/%Y'))\n",
    "\n",
    "# Load different prompt templates from files.\n",
    "with open(\"./prompts/prompt_with_nocontext.txt\", \"r\") as f:\n",
    "    prompt_with_nocontext = f.read().strip()\n",
    "\n",
    "with open(\"./prompts/prompt_with_context.txt\", \"r\") as f:\n",
    "    prompt_template = f.read().strip()\n",
    "\n",
    "with open(\"./prompts/dspy_template.txt\", \"r\") as f:\n",
    "    dspy_template = f.read().strip()\n",
    "\n",
    "# Function to format the prompt using a template that includes context.\n",
    "def format_prompt(doc):\n",
    "    return prompt_template.format(\n",
    "        Case_Inputs=json.dumps(doc.get('Case_Inputs', ''), indent=1)[2:-2],\n",
    "        Citation_context=json.dumps(doc.get('Citation_context', ''), indent=1)[2:-2],\n",
    "        Similar_Cases_Analysis=json.dumps(doc.get('Similar_Cases_Analysis', ''), indent=1)[2:-2]\n",
    "    )\n",
    "\n",
    "# Function to format the prompt using a template from DSpy.\n",
    "def dspy_format_prompt(doc):\n",
    "    return dspy_template.format(\n",
    "        Case_Inputs=json.dumps(doc.get('Case_Inputs', ''), indent=1)[2:-2],\n",
    "        Citation_context=json.dumps(doc.get('Citation_context', ''), indent=1)[2:-2],\n",
    "        Similar_Cases_Analysis=json.dumps(doc.get('Similar_Cases_Analysis', ''), indent=1)[2:-2]\n",
    "    )\n",
    "\n",
    "# Function to format the prompt without any context.\n",
    "def format_prompt_nocontext(doc):\n",
    "    return prompt_with_nocontext.format(\n",
    "        Case_Inputs=json.dumps(doc.get('Case_Inputs', ''), indent=1)[2:-2]\n",
    "    )\n",
    "\n",
    "# Initialize the Together API client with an API key.\n",
    "tg.api_key = \"Enter your Together API key\"\n",
    "tg.api_base = \"https://api.together.xyz/v1\"\n",
    "\n",
    "# Function to generate text using the Together API.\n",
    "def generate_text(prompt, max_tokens=800):\n",
    "    response = tg.Completion.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",  # Baseline Model -> Will only run it with the First prompt (no added context)\n",
    "        #model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",  # Uncomment to use Mistral-8x7B\n",
    "        prompt=prompt,  # The formatted prompt to generate text from\n",
    "        max_tokens=max_tokens,  # Maximum number of tokens to generate\n",
    "        n=1,  # Number of completions to generate\n",
    "        stop=None,  # Stopping criteria for the generation\n",
    "        temperature=1.0  # Controls the randomness of the generation\n",
    "    )\n",
    "    return response.choices[0].text.strip()  # Return the generated text\n",
    "\n",
    "# Load existing generated analyses if the JSON file exists.\n",
    "if os.path.exists('./results/generated_analyses.json'):\n",
    "    with open('./results/generated_analyses.json', 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "else:\n",
    "    existing_data = [] \n",
    "\n",
    "# Initialize lists to store results and references for evaluation.\n",
    "results = []\n",
    "references = []\n",
    "generated_texts = []\n",
    "\n",
    "# Create a lookup dictionary for existing data by case ID.\n",
    "existing_data_lookup = {case['_id']: case for case in existing_data}\n",
    "\n",
    "# Loop through the cases to generate analyses using the Together API.\n",
    "for case in tqdm(f_cases_data, desc='Generating Analysis'):\n",
    "    formatted_prompt = format_prompt_nocontext(case)  # First: Using the no-context prompt format initially\n",
    "    #formatted_prompt = format_prompt(case)           # Second: Uncomment to use with context prompt format\n",
    "    #formatted_prompt = dspy_format_prompt(case)      # Third: Created using dspy framework\n",
    "    \n",
    "    # Generate analysis using the Together API\n",
    "    analysis = generate_text(formatted_prompt, max_tokens=800)\n",
    "    \n",
    "    case_id = case['_id']\n",
    "    \n",
    "    # Updating existing data or adding new analysis.\n",
    "    if case_id in existing_data_lookup:\n",
    "        # Update the existing case with the new analysis\n",
    "        existing_data_lookup[case_id]['Gen_Analysis_TG_llama_NC'] = analysis  # Store in column named Gen_Analysis_TG_MST_NC, 'Gen_Analysis_TG_MST_WC', 'Gen_Analysis_TG_MST_dspy' while using Mistral with First, Second and Third prompt respectively \n",
    "    else:\n",
    "        # Add the generated analysis to the new case\n",
    "        case['Gen_Analysis_TG_llama_NC'] = analysis                           # Store in column named Gen_Analysis_TG_MST_NC, 'Gen_Analysis_TG_MST_WC', 'Gen_Analysis_TG_MST_dspy' while using Mistral with First, Second and Third prompt respectively \n",
    "        existing_data.append(case)  # Append new case to the list\n",
    "    \n",
    "    # Store reference and generated texts for evaluation.\n",
    "    references.append(case['Case_Result'])\n",
    "    generated_texts.append(analysis)\n",
    "\n",
    "# Save the generated analyses to a JSON file.\n",
    "with open('./results/generated_analyses.json', 'w') as f:\n",
    "    json.dump(existing_data, f, indent=2)\n",
    "\n",
    "print(\"Analysis Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e82b64-e99c-486d-b2ea-c19f0887b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation using various metrics.\n",
    "\n",
    "# Evaluate using METEOR\n",
    "meteor = evaluate.load('meteor')\n",
    "meteor_results = meteor.compute(predictions=generated_texts, references=references)\n",
    "print(\"METEOR Score:\", meteor_results)\n",
    "\n",
    "# Evaluate using BERTScore\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bertscore_results = bertscore.compute(predictions=generated_texts, references=references, lang=\"en\")\n",
    "#print(\"BERTScore Results:\", bertscore_results)\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "rouge = evaluate.load('rouge')\n",
    "rouge_results = rouge.compute(predictions=generated_texts, references=references, use_stemmer=True)\n",
    "print(\"ROUGE Results:\", rouge_results)\n",
    "\n",
    "# Convert the BERTScore F1 scores to a numpy array and calculate the average.\n",
    "scores_array = np.array(bertscore_results['f1'])\n",
    "average_score = np.nanmean(scores_array)\n",
    "print(f\"Avg F1 Score: {average_score}\")\n",
    "\n",
    "# Calculate the average precision score from BERTScore results.\n",
    "p_scores_array = np.array(bertscore_results['precision'])\n",
    "p_average_score = np.nanmean(p_scores_array)\n",
    "print(f\"Avg Precision Score: {p_average_score}\")\n",
    "\n",
    "# Calculate the average recall score from BERTScore results.\n",
    "r_scores_array = np.array(bertscore_results['recall'])\n",
    "r_average_score = np.nanmean(r_scores_array)\n",
    "print(f\"Avg Recall Score: {r_average_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
