{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756d4281-3889-4e1b-a311-6ff1c0bcf3b1",
   "metadata": {},
   "source": [
    "# Section 1: Preprocessing\n",
    "This section involves all the steps related to data cleaning, formatting, and preparing the dataset.\n",
    "The goal is to ensure that the input data is in the right format for the model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f54e55-44ed-4021-b0ee-bfde2c69520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-gpu\n",
    "#!pip install pdfplumber\n",
    "#!pip install transformers==4.44.0\n",
    "#!pip install accelerate==0.33.0\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import faiss\n",
    "import pdfplumber\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Function to extract and clean text from a PDF document\n",
    "def extract_and_clean_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        raw_text = []\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                raw_text.append(text)\n",
    "    return raw_text\n",
    "\n",
    "# Function to further clean the extracted text\n",
    "def clean_extracted_text(raw_text):\n",
    "    cleaned_text = []\n",
    "    for text in raw_text:\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "        text = re.sub(r'[^A-Za-z0-9.,;:()\\\\/\\-\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "        text = re.sub(r'\\d+\\^\\d+', '', text)  # Remove superscript numbers\n",
    "        text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)  # Fix broken words\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to split cleaned text into sections based on patterns\n",
    "def split_into_sections(cleaned_text):\n",
    "    sections = []\n",
    "    current_section = []\n",
    "    section_pattern = re.compile(r'\\b(?:CHAPTER|Section|SECTION|SCHEDULE)\\s+\\d+\\b', re.IGNORECASE)\n",
    "    title_pattern = re.compile(r'^\\s*(\\d+)\\s*[-–—]\\s*(.*)$')\n",
    "    paragraph_pattern = re.compile(r'^\\(\\d+\\)')  # Pattern for numbered list points\n",
    "\n",
    "    for text in cleaned_text:\n",
    "        lines = text.split('. ')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if section_pattern.match(line):\n",
    "                if current_section:\n",
    "                    sections.append(' '.join(current_section))\n",
    "                    current_section = []\n",
    "            elif paragraph_pattern.match(line):\n",
    "                if current_section:\n",
    "                    sections.append(' '.join(current_section))\n",
    "                    current_section = []\n",
    "            title_match = title_pattern.match(line)\n",
    "            if title_match:\n",
    "                current_section.append(f\"Section {title_match.group(1)}: {title_match.group(2)}\")\n",
    "            else:\n",
    "                current_section.append(line)\n",
    "        if current_section:\n",
    "            sections.append(' '.join(current_section))\n",
    "            current_section = []\n",
    "    return sections\n",
    "\n",
    "# Extract text from the PDF document of the Negotiable Instruments Act\n",
    "pdf_path = './data/The Negotiable Instruments Act, 1881.pdf'\n",
    "raw_text = extract_and_clean_text_from_pdf(pdf_path)\n",
    "\n",
    "# Clean the extracted text\n",
    "cleaned_text = clean_extracted_text(raw_text)\n",
    "\n",
    "# Split cleaned text into sections\n",
    "sections = split_into_sections(cleaned_text)\n",
    "\n",
    "# File path for the JSON data to be processed\n",
    "nia_cases_to_process_file = './data/nia_cases_to_process.json'\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.exists(nia_cases_to_process_file)\n",
    "\n",
    "# If the file doesn't exist, filter and save the cases\n",
    "if not file_exists:\n",
    "    # Load JSON data from file\n",
    "    with open('./data/cases.nia_cases.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter the cases based on the presence of required fields\n",
    "    filtered_cases = [case for case in data if \"RLC\" in case and \"RPC\" in case and \"Facts\" in case and \"Arg_Pet\" in case and \"Arg Resp\" in case and \"Formatted_JudgementText\" in case and \"JudgmentDate\" in case and \"Analysis\" in case]\n",
    "\n",
    "    # Save the filtered cases to a new JSON file\n",
    "    with open('./data/cleaned_nia_cases.json', 'w') as outfile:\n",
    "        json.dump(filtered_cases, outfile, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(filtered_cases)} cases to 'cleaned_nia_cases.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd665f83-1ab0-4661-8c15-55fc6bff2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered cases JSON file\n",
    "cases_data = load_json('./data/cleaned_nia_cases.json')\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\r\\n', '\\n', text)  # Convert \\r\\n to \\n\n",
    "    text = re.sub(r'(?<![\\.\\!\\?])\\n', ' ', text)  # Remove newlines not at end of sentences\n",
    "    text = re.sub(r'[ \\t]+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    return text\n",
    "\n",
    "# Function to combine a list of text fields into a single cleaned string\n",
    "def combine_and_clean_text(text_list):\n",
    "    combined_text = ' '.join(text_list)  # Combine list into a single string\n",
    "    return clean_text(combined_text)  # Clean the combined text\n",
    "    \n",
    "# Clean specific text fields in the case data\n",
    "if 'JudgmentText' in cases_data:\n",
    "    cases_data['JudgmentText'] = [clean_text(para) for para in cases_data['JudgmentText']]\n",
    "for key in ['Facts', 'RLC', 'Analysis', 'RPC','Arg Resp','Arg_Pet','Result']:\n",
    "    if key in cases_data:\n",
    "        cases_data[key] = [combine_and_clean_text(para) for para in cases_data[key]]\n",
    "\n",
    "# Function to extract citations from a given text\n",
    "def extract_citations(text: str) -> List[str]:\n",
    "    pattern = r'Section \\d+'  # Match sections like \"Section 138\" or \"section 138\"\n",
    "    citations = re.findall(pattern, text, re.IGNORECASE)  # Case-insensitive search\n",
    "    citations = [citation.lower() for citation in citations]  # Convert all to lowercase for uniformity\n",
    "    return citations\n",
    "\n",
    "# Function to extract citations from specific sections of a case\n",
    "def extract_from_sections(case: dict) -> Dict[str, List[str]]:\n",
    "    sections = ['Facts', 'Arg_Pet', 'Arg Resp', 'RLC']\n",
    "    citations_dict = {}\n",
    "    for section in sections:\n",
    "        if section in case:\n",
    "            texts = case[section]\n",
    "            all_citations = []\n",
    "            for text in texts:\n",
    "                all_citations.extend(extract_citations(text))\n",
    "            citations_dict[section] = all_citations\n",
    "    return citations_dict\n",
    "\n",
    "# Function to extract unique citations from the sections\n",
    "def extract_unique_citations(citations):\n",
    "    unique_citations = set()\n",
    "    for sections in citations.values():\n",
    "        unique_citations.update(sections)\n",
    "    return list(unique_citations)\n",
    "\n",
    "# Summarize and process the case data if the file doesn't already exist\n",
    "if not file_exists:\n",
    "    # Check if GPU is available and select the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize a summarization pipeline using a pretrained BART model\n",
    "    summarization_pipeline = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=device)\n",
    "\n",
    "    # Function to summarize a list of texts using the pipeline\n",
    "    def summarize_texts(texts, max_length=150, min_length=50, batch_size=10):\n",
    "        summaries = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            summaries.extend(summarization_pipeline(batch_texts, max_length=max_length, min_length=min_length, do_sample=False))\n",
    "        return [summary['summary_text'] for summary in summaries]\n",
    "\n",
    "    # Convert the sections into a Hugging Face dataset for processing\n",
    "    dataset = Dataset.from_dict({\"section_text\": sections})\n",
    "\n",
    "    # Apply the summarization function to the dataset in batches\n",
    "    def summarize_batch(batch):\n",
    "        batch['summarized_text'] = summarize_texts(batch['section_text'], max_length=150, min_length=50, batch_size=10)\n",
    "        return batch\n",
    "\n",
    "    # Process and summarize the sections\n",
    "    summarized_dataset = dataset.map(summarize_batch, batched=True, batch_size=10)\n",
    "\n",
    "    # Convert summarized sections to a list\n",
    "    summarized_sections = summarized_dataset['summarized_text']\n",
    "\n",
    "    # Function to retrieve sections based on query text\n",
    "    def retrieve_exact_sections(sections, query_text):\n",
    "        filtered_paragraphs = [paragraph for paragraph in sections if any(name.lower() in paragraph.lower() for name in query_text)]\n",
    "        return filtered_paragraphs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfa503-d9b3-4728-a80e-4034014e9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed cases and search for similar ones if the file doesn't exist\n",
    "if not file_exists:\n",
    "\n",
    "    # Initialize the tokenizer and model for embedding\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Function to embed text using the tokenizer and model\n",
    "    def embed_texts(texts, batch_size=16):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = embedding_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
    "            embedding_model.to('cuda')  \n",
    "            with torch.no_grad():\n",
    "                outputs = embedding_model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    # Function to convert lists to strings\n",
    "    def convert_to_string(value):\n",
    "        if isinstance(value, list):\n",
    "            return ' '.join(value)\n",
    "        return value\n",
    "\n",
    "    # Summarize similar cases\n",
    "    def summarize_similar_cases(text, max_length=200, min_length=50):\n",
    "        if len(text) > 200:\n",
    "            summary = summarization_pipeline(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        return text\n",
    "\n",
    "    # Function to create a FAISS index for searching similar cases\n",
    "    def create_faiss_index(cases_data, batch_size=16):\n",
    "        case_embeddings = []\n",
    "        for case in tqdm(cases_data, desc=\"Processing Cases\"):\n",
    "            text_to_embed = (convert_to_string(case.get('Facts', '')) + ' ' +\n",
    "                         convert_to_string(case.get('RLC', '')) + ' ' +\n",
    "                         convert_to_string(case.get('Arg_Pet', '')) + ' ' +\n",
    "                         convert_to_string(case.get('Arg Resp', '')))\n",
    "            case_embeddings.append(text_to_embed)\n",
    "    \n",
    "        # Embed the texts in batches\n",
    "        embeddings = embed_texts(case_embeddings, batch_size=batch_size)\n",
    "    \n",
    "        # Create FAISS index\n",
    "        faiss_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        faiss_index.add(embeddings)\n",
    "    \n",
    "        return faiss_index, embeddings\n",
    "\n",
    "    # Create the FAISS index and precompute embeddings\n",
    "    faiss_index, all_case_embeddings = create_faiss_index(cases_data, batch_size=16)\n",
    "\n",
    "    # Function to fetch similar cases using the FAISS index\n",
    "    def fetch_similar_cases(case, k=4):\n",
    "        text_to_embed = (convert_to_string(case.get('Facts', '')) + ' ' +\n",
    "                         convert_to_string(case.get('RLC', '')) + ' ' +\n",
    "                         convert_to_string(case.get('Arg_Pet', '')) + ' ' +\n",
    "                         convert_to_string(case.get('Arg Resp', '')))\n",
    "        query_embedding = embed_texts([text_to_embed])\n",
    "\n",
    "        D, I = faiss_index.search(query_embedding, k)\n",
    "        similar_cases = [{'_id': cases_data[i]['_id'],\n",
    "                          'Facts': summarize_similar_cases(cases_data[i].get('Facts', '')),\n",
    "                          'RLC': summarize_similar_cases(cases_data[i].get('RLC', '')),\n",
    "                          'Arg_Pet': summarize_similar_cases(cases_data[i].get('Arg_Pet', '')),\n",
    "                          'Arg_Resp': summarize_similar_cases(cases_data[i].get('Arg Resp', '')),\n",
    "                          'Citation_context': summarize_similar_cases(cases_data[i].get('Citation_context', '')),\n",
    "                          'Analysis': cases_data[i].get('Analysis', ''),\n",
    "                          'JudgmentDate': cases_data[i].get('JudgmentDate', '')}\n",
    "                         for i in I[0]]\n",
    "    \n",
    "        # Filter out cases with judgment dates less than the current case\n",
    "        current_judgement_date = case['JudgmentDate']\n",
    "        similar_cases = [c for c in similar_cases if c['JudgmentDate'] < current_judgement_date]\n",
    "\n",
    "        return similar_cases[:1]\n",
    "    \n",
    "    # Extract citations, summarize them, and create the 'Similar_Case_Analysis' & 'Citation_context' fields\n",
    "    for case in tqdm(cases_data, desc=\"Extracting and Summarizing Citations\"):\n",
    "        citations = extract_from_sections(case)\n",
    "        unique_citations = extract_unique_citations(citations)\n",
    "        exact_sections = retrieve_exact_sections(summarized_sections, unique_citations)\n",
    "        case['Citation_context'] = exact_sections\n",
    "        \n",
    "    # Add similar case analysis to each case\n",
    "    for case in tqdm(cases_data, desc=\"Adding Similar Cases Analysis\"):\n",
    "        similar_cases = fetch_similar_cases(case)\n",
    "        similar_cases_str = \"\"\n",
    "        for i, sim_case in enumerate(similar_cases):\n",
    "            similar_cases_str += f\"\\n### **Similar Case {i+1}:**\\n\"\n",
    "            similar_cases_str += f\"**Case ID**: {sim_case.get('_id', '')}\\n\"\n",
    "            similar_cases_str += f\"**Facts**: {convert_to_string(sim_case.get('Facts', ''))}\\n\"\n",
    "            similar_cases_str += f\"**Ruling by Lower Court**: {convert_to_string(sim_case.get('RLC', ''))}\\n\"\n",
    "            similar_cases_str += f\"**Argument by Petitioner**: {convert_to_string(sim_case.get('Arg_Pet', ''))}\\n\"\n",
    "            similar_cases_str += f\"**Argument by Respondent**: {convert_to_string(sim_case.get('Arg_Resp', ''))}\\n\"\n",
    "            similar_cases_str += f\"**Citation Context**: {convert_to_string(sim_case.get('Citation_context', ''))}\\n\"\n",
    "            similar_cases_str += f\"**Analysis**: {convert_to_string(sim_case.get('Analysis', ''))}\\n\"\n",
    "        case['Similar_Cases_Analysis'] = similar_cases_str\n",
    "\n",
    "# Concatenate the relevant sections into a single field for each case\n",
    "if not file_exists:\n",
    "\n",
    "    def concat_case_inputs(doc):\n",
    "        key_to_name = {\n",
    "            'Facts': \"Facts\",\n",
    "            'Arg_Pet': \"Argument by the Petitioner\",\n",
    "            'Arg Resp': \"Argument by the Respondent\",\n",
    "            'RLC': \"Ruling by the Lower Court\"\n",
    "        }\n",
    "        record = \"\"\n",
    "        for key in ['Facts', 'Arg_Pet', 'Arg Resp', 'RLC']:\n",
    "            if key in doc:\n",
    "                record += f\"{key_to_name[key]}:\\n\"\n",
    "                record += \" \".join(doc[key]) + \"\\n\"\n",
    "        return record\n",
    "\n",
    "    # Create Case_Result field that is the concatenation of the Analysis and Result fields\n",
    "    def concat_outputs(doc):\n",
    "        analysis = \"\"\n",
    "        for key in ['Analysis', 'Result']:\n",
    "            if key in doc:\n",
    "                content = \"\".join(doc[key]).replace(\"\\\\n\", \"\\n\").strip()\n",
    "                # Concatenate the cleaned content\n",
    "                analysis += content + \" \"\n",
    "        return analysis.strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "    # Process the documents, keeping only necessary fields\n",
    "    filtered_cases = []\n",
    "    for doc in cases_data:\n",
    "        processed_doc = {\n",
    "            '_id': str(doc['_id']),\n",
    "            'JudgmentDate': doc.get('JudgmentDate', ''),\n",
    "            'Case_Inputs': concat_case_inputs(doc),\n",
    "            'Case_Result': concat_outputs(doc),\n",
    "            'Citation_context': doc.get('Citation_context', ''),\n",
    "            'Similar_Cases_Analysis': doc.get('Similar_Cases_Analysis', '')\n",
    "        }\n",
    "        filtered_cases.append(processed_doc)\n",
    "\n",
    "    # Save the filtered cases to a file\n",
    "    with open('./data/nia_cases_to_process.json', 'w') as f:\n",
    "        for doc in filtered_cases:\n",
    "            f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "    print(\"Filtered documents saved to nia_cases_to_process.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
