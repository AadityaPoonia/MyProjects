{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72c34c2-da6e-4b26-a4b8-48de4e56ff31",
   "metadata": {},
   "source": [
    "# Section 2: Prompting using DSpy\n",
    "This section will include the functions or methods needed for creating prompts using DSpy.\n",
    "The prompts are created to structure the input data before passing it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04528f-3380-4825-a8b5-54d7c6cb73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dspy-ai\n",
    "\n",
    "import dspy\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPRO\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import accelerate\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load the JSON data containing the case details\n",
    "cases_data = load_json('./data/cleaned_nia_cases.json')\n",
    "\n",
    "# Set up quantization configuration for model loading to improve memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization for better memory efficiency\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computations\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization for improved accuracy\n",
    ")\n",
    "\n",
    "# Initialize the HF model using DSPy HFModel class with quantization settings\n",
    "dspy_model = dspy.HFModel(\n",
    "    model='mistralai/Mixtral-8x7B-Instruct-v0.1',  # Model ID\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": quantization_config,  # Apply the quantization settings\n",
    "        \"device_map\": \"auto\",  # Automatically distribute model across available devices\n",
    "        \"token\": \"Enter your HF token\"  # Access token for Hugging Face API\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure DSPy with the initialized HF model\n",
    "dspy.settings.configure(lm=dspy_model)\n",
    "\n",
    "# Set up the device for PyTorch operations \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'\n",
    "torch.cuda.memory_summary()\n",
    "\n",
    "# Define a custom evaluation metric function combining BERTScore and ROUGE\n",
    "def custom_metric_evaluation(original, generated, trace=None):\n",
    "    \n",
    "    # Calculate BERTScore\n",
    "    P, R, F1 = bert_score([original.petition_verdict], [generated.petition_verdict], lang=\"en\", verbose=True,device=device)\n",
    "\n",
    "    # Calculate ROUGE-L score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(original.petition_verdict, generated.petition_verdict)\n",
    "    rouge_f1 = scores['rougeL'].fmeasure  # Using ROUGE-L F1 score\n",
    "\n",
    "    # Combine BERTScore F1 and ROUGE-L F1 with weighted average\n",
    "    combined_score = 0.15*F1.mean().item() + 0.85*rouge_f1\n",
    "    return combined_score\n",
    "\n",
    "# Prepare input text by converting list input to a single string if necessary\n",
    "def prepare_text_input(input_value):\n",
    "    if isinstance(input_value, list):\n",
    "        return \" \".join(input_value)  # Convert list to a single string\n",
    "    return input_value  \n",
    "\n",
    "# Guidelines for generating legal analysis, provided as context to the model\n",
    "guidelines = \"\"\"\n",
    "You are a seasoned Indian High Court judge specializing in banking law, particularly the Indian Negotiable Instruments Act.\n",
    "Follow these guidelines:\n",
    "1. Analyze the legal arguments concisely.\n",
    "2. Refer to past cases and applicable laws.\n",
    "3. Provide a final verdict, and conclude with a single word: 'ALLOWED,' 'DISMISSED,' or 'OTHER.'\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the dataset by formatting each case according to the guidelines and inputs\n",
    "dataset = [\n",
    "    dspy.Example(\n",
    "        legal_case_facts=prepare_text_input(case['Facts']),\n",
    "        arguments_by_respondent=prepare_text_input(case['Arg Resp']),\n",
    "        argument_by_petitioner=prepare_text_input(case['Arg_Pet']),\n",
    "        ruling_by_lower_court=prepare_text_input(case['RLC']),\n",
    "        petition_verdict=prepare_text_input(case['Analysis']),\n",
    "        guidelines=guidelines\n",
    "    ).with_inputs(\"legal_case_facts\", \"arguments_by_respondent\", \"argument_by_petitioner\", \"ruling_by_lower_court\", \"guidelines\")\n",
    "    for case in cases_data[:5]  # Limit to first 5 cases due to memory constraints\n",
    "]\n",
    "\n",
    "# Define the chain of thought model input and output mapping\n",
    "LegalAnalysis = dspy.ChainOfThought(\"guidelines,legal_case_facts,arguments_by_respondent,argument_by_petitioner,ruling_by_lower_court -> petition_verdict\")\n",
    "\n",
    "# Use MIPRO optimizer for prompt tuning with specified configuration\n",
    "config = dict(num_candidates=4)\n",
    "optimizer = MIPRO(metric=custom_metric_evaluation, **config)\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Compile the program with optimization and evaluation settings\n",
    "compiled_program = optimizer.compile(\n",
    "    LegalAnalysis,\n",
    "    trainset=dataset, \n",
    "    num_trials=3,\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=2,\n",
    "    eval_kwargs=kwargs,\n",
    "    requires_permission_to_run=False,\n",
    "    view_data=False,\n",
    "    view_examples=False\n",
    ")\n",
    "\n",
    "# Save the optimized prompt configuration for future use\n",
    "compiled_program.save(\"./results/optimized_promptv2\")\n",
    "print(compiled_program)\n",
    "\n",
    "print(\"Analysis Generated and Saved with Optimized Prompts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
